{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Experiment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-augmented generation (RAG) is a technique that improves the quality of large language model (LLM) outputs by grounding the model on external sources of knowledge. RAG works by first retrieving a set of relevant documents from a knowledge base, such as documents stored in a vector database, in response to a given prompt. The retrieved documents are then concatenated with the original prompt and fed to the LLM, which uses them to generate a more informed and accurate response.\n",
    "\n",
    "As seen from other notebook examples, PromptTools enables you test various LLMs and vector databases independently. For [example](https://github.com/hegelai/prompttools/blob/main/examples/notebooks/vectordb_experiments/ChromaDBExperiment.ipynb), you can provide a prompt to `ChromaDB` to see if the list of returned documents are sufficiently relevant. In this example, we combine the evaluation of vector databases and LLMs and evaluate the final outputs of the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.experiment import ChromaDBExperiment, OpenAICompletionExperiment, OpenAIChatExperiment\n",
    "from prompttools.harness import RetrievalAugmentedGenerationExperimentationHarness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using OpenAI's LLM in this example. You can set up your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Put your key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data for retrieval with a VectorDB Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main steps in Retrieval Augmented Generation. We will start with the first step - retrieval.\n",
    "\n",
    "We will set up a vector database experiment. We will insert documents the DB with different embedding functions (vectorizer), and query the results.\n",
    "\n",
    "For this example, we will use ChromaDB, but you use other vector databases (e.g. Weaviate, LanceDB, Qdrant) as well. You can also experiment over different distance function and query methods if desired.\n",
    "\n",
    "For detailed explanation about each step, have a look at the [ChromaDB example notebook](https://github.com/hegelai/prompttools/blob/main/examples/notebooks/vectordb_experiments/ChromaDBExperiment.ipynb). It also discusses how you can try different chunk and pre-processing strategies as you insert documents into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "emb_fns = [\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"paraphrase-MiniLM-L3-v2\"),\n",
    "    embedding_functions.DefaultEmbeddingFunction(),\n",
    "]  # default is \"all-MiniLM-L6-v2\"\n",
    "emb_fns_names = [\"paraphrase-MiniLM-L3-v2\", \"default\"]\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "# You can also create and use `chromadb.PersistentClient` or `chromadb.HttpClient`\n",
    "TEST_COLLECTION_NAME = \"TEMPORARY_COLLECTION\"\n",
    "try:\n",
    "    chroma_client.delete_collection(TEST_COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "collection_name = TEST_COLLECTION_NAME\n",
    "\n",
    "use_existing_collection = False  # Specify that we want to create a collection during the experiment\n",
    "\n",
    "# Documents that will be added into the database\n",
    "add_to_collection_params = {\n",
    "    \"documents\": [\"Mickey Mouse is the 50th president.\",\n",
    "                  \"The 51st president is Snoopy.\",\n",
    "                  \"Batman became the 52th president briefly after.\"],\n",
    "    \"metadatas\": [{\"source\": \"my_source\"}, {\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n",
    "    \"ids\": [\"id1\", \"id2\", \"id3\"],\n",
    "}\n",
    "\n",
    "# Our test queries\n",
    "query_collection_params = {\"query_texts\": [\"Who was the 50th president?\", \"Who was the 51st president?\"],\n",
    "                           \"n_results\": [1],  # You can have the model returns more results if you'd like\n",
    "                          }\n",
    "\n",
    "\n",
    "# Set up the experiment\n",
    "vdb_experiment = ChromaDBExperiment(\n",
    "    chroma_client,\n",
    "    collection_name,\n",
    "    use_existing_collection,\n",
    "    query_collection_params,\n",
    "    emb_fns,\n",
    "    emb_fns_names,\n",
    "    add_to_collection_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results and see what documents have been fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_texts</th>\n",
       "      <th>embed_fn</th>\n",
       "      <th>top doc ids</th>\n",
       "      <th>distances</th>\n",
       "      <th>documents</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who was the 50th president?</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id2]</td>\n",
       "      <td>[21.199106216430664]</td>\n",
       "      <td>[The 51st president is Snoopy.]</td>\n",
       "      <td>0.007107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the 51st president?</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id2]</td>\n",
       "      <td>[13.693190574645996]</td>\n",
       "      <td>[The 51st president is Snoopy.]</td>\n",
       "      <td>0.005465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who was the 50th president?</td>\n",
       "      <td>default</td>\n",
       "      <td>[id1]</td>\n",
       "      <td>[0.617713212966919]</td>\n",
       "      <td>[Mickey Mouse is the 50th president.]</td>\n",
       "      <td>0.019721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who was the 51st president?</td>\n",
       "      <td>default</td>\n",
       "      <td>[id2]</td>\n",
       "      <td>[0.6116843819618225]</td>\n",
       "      <td>[The 51st president is Snoopy.]</td>\n",
       "      <td>0.019985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   query_texts                 embed_fn top doc ids  \\\n",
       "0  Who was the 50th president?  paraphrase-MiniLM-L3-v2  [id2]        \n",
       "1  Who was the 51st president?  paraphrase-MiniLM-L3-v2  [id2]        \n",
       "2  Who was the 50th president?  default                  [id1]        \n",
       "3  Who was the 51st president?  default                  [id2]        \n",
       "\n",
       "              distances                              documents   latency  \n",
       "0  [21.199106216430664]  [The 51st president is Snoopy.]        0.007107  \n",
       "1  [13.693190574645996]  [The 51st president is Snoopy.]        0.005465  \n",
       "2  [0.617713212966919]   [Mickey Mouse is the 50th president.]  0.019721  \n",
       "3  [0.6116843819618225]  [The 51st president is Snoopy.]        0.019985  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vdb_experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first embedding function returns \"The 51st president is Snoopy\" for both queries. This inaccuracy is going to cause problem as we pass the documents into the LLM, because it will not have the right context to answer questions.\n",
    "\n",
    "At this point, you have results from the retrieval step. If you wish to evaluate how relevant the retrieved documents are, you can. The [ChromaDB notebook example](https://github.com/hegelai/prompttools/blob/main/examples/notebooks/vectordb_experiments/ChromaDBExperiment.ipynb) shows you how you may do that. We will skip that here for brevity.\n",
    "\n",
    "It is often worthwhile to independently evaluate the retrieval step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup up Retrieval Augmented Generation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up your vector database experiment, we can set up the LLM experiment that will consume the documents retrieved from the vector DB. We need:\n",
    "\n",
    "1. LLM experiment (we will use `OpenAICompletionExperiment` here, but you can use something else as well)\n",
    "2. LLM arguments (this will be passed into the LLM experiment)\n",
    "3. A function to extract documents from the resuls of the vector DB experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the arguments we will use for our LLM experiment `OpenAICompletionExperiment`. For an example with `OpenAIChatExperiment` (that uses `gpt-3.5-turbo`, scroll further below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"babbage-002\"]  # If you want to use \"gpt-3,5-turbo\", look further below for an example\n",
    "prompts = [\"Who is the 50th president?\", \"Who is the 51st president?\"]\n",
    "temperatures = [1.0]  # You can test multiple temperate or other parameters as wel\n",
    "# You can add more parameters that you'd like to test here.\n",
    "\n",
    "llm_arguments = {\"model\": models, \"prompt\": prompts, \"temperature\": temperatures}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two functions:\n",
    "1. Extracts the list of documents from each row of the vector DB experiment result. These lists will be passed to the LLM during the generation process.\n",
    "2. Generate a string of relevant metadata based on the row. This will be used later for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_doc_from_row(row: 'pandas.core.series.Series'):\n",
    "    return row['documents']\n",
    "\n",
    "\n",
    "def _extract_query_metadata_from_row(row: 'pandas.core.series.Series'):\n",
    "    return f\"emb_fn: {row['embed_fn']}, prompt: {row['query_texts']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass in everything into the RAG experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_experiment = RetrievalAugmentedGenerationExperimentationHarness(\n",
    "    vector_db_experiment = vdb_experiment,\n",
    "    llm_experiment_cls = OpenAICompletionExperiment,\n",
    "    llm_arguments = llm_arguments,\n",
    "    extract_document_fn = _extract_doc_from_row,\n",
    "    extract_query_metadata_fn = _extract_query_metadata_from_row,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>latency</th>\n",
       "      <th>retrieval_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?</td>\n",
       "      <td>1 Like her bear.\\nview more\\n\\nLucas Cook is asking for help</td>\n",
       "      <td>0.316887</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?</td>\n",
       "      <td>What is the photo tag?\\n\\nSnoopy's aversion to being called a</td>\n",
       "      <td>0.200568</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?</td>\n",
       "      <td>[The 49th and 50th presidents] are Bobo.\\n\\nWho</td>\n",
       "      <td>0.187423</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?</td>\n",
       "      <td>Tricky — and if you stick to the facts, you might figure it out</td>\n",
       "      <td>0.186136</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Given these documents:\\nMickey Mouse is the 50th president.\\n\\nWho is the 50th president?</td>\n",
       "      <td>[/color]\\n\\nOkay. So that pretty much answers that.'\\n\\nNow, we just</td>\n",
       "      <td>0.217461</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Given these documents:\\nMickey Mouse is the 50th president.\\n\\nWho is the 51st president?</td>\n",
       "      <td>To get answers to these questions and others like it, Disney has produced a historical</td>\n",
       "      <td>0.192069</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?</td>\n",
       "      <td>See the answer with this poem.\\nCorrect answer is Ronald Reagan \\n\\nWhich 200</td>\n",
       "      <td>0.182476</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?</td>\n",
       "      <td>\\nThe 51st president thereby ceased to be president when he wrote the document</td>\n",
       "      <td>0.193127</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      prompt  \\\n",
       "0  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?         \n",
       "1  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?         \n",
       "2  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?         \n",
       "3  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?         \n",
       "4  Given these documents:\\nMickey Mouse is the 50th president.\\n\\nWho is the 50th president?   \n",
       "5  Given these documents:\\nMickey Mouse is the 50th president.\\n\\nWho is the 51st president?   \n",
       "6  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 50th president?         \n",
       "7  Given these documents:\\nThe 51st president is Snoopy.\\n\\nWho is the 51st president?         \n",
       "\n",
       "                                                                                  response  \\\n",
       "0   1 Like her bear.\\nview more\\n\\nLucas Cook is asking for help                             \n",
       "1   What is the photo tag?\\n\\nSnoopy's aversion to being called a                            \n",
       "2   [The 49th and 50th presidents] are Bobo.\\n\\nWho                                          \n",
       "3   Tricky — and if you stick to the facts, you might figure it out                          \n",
       "4   [/color]\\n\\nOkay. So that pretty much answers that.'\\n\\nNow, we just                     \n",
       "5   To get answers to these questions and others like it, Disney has produced a historical   \n",
       "6   See the answer with this poem.\\nCorrect answer is Ronald Reagan \\n\\nWhich 200            \n",
       "7   \\nThe 51st president thereby ceased to be president when he wrote the document           \n",
       "\n",
       "    latency  \\\n",
       "0  0.316887   \n",
       "1  0.200568   \n",
       "2  0.187423   \n",
       "3  0.186136   \n",
       "4  0.217461   \n",
       "5  0.192069   \n",
       "6  0.182476   \n",
       "7  0.193127   \n",
       "\n",
       "                                                     retrieval_metadata  \n",
       "0  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  \n",
       "1  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  \n",
       "2  emb_fn: default, prompt: Who was the 50th president?                  \n",
       "3  emb_fn: default, prompt: Who was the 51st president?                  \n",
       "4  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  \n",
       "5  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  \n",
       "6  emb_fn: default, prompt: Who was the 50th president?                  \n",
       "7  emb_fn: default, prompt: Who was the 51st president?                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_experiment.run()\n",
    "rag_experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `\"babbage-002\"` is not very good despite having the right documents/context. We will use GPT-3.5 next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_models = [\"gpt-3.5-turbo\"]\n",
    "messages = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who was the 50th president?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who was the 51st president?\"},\n",
    "    ]\n",
    "]\n",
    "temperatures = [1.0]  # You can test multiple temperate or other parameters as wel\n",
    "# You can add more parameters that you'd like to test here.\n",
    "\n",
    "llm_chat_arguments = {\"model\": chat_models, \"messages\": messages, \"temperature\": temperatures}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>response</th>\n",
       "      <th>latency</th>\n",
       "      <th>retrieval_metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given document that states \"The 51st president is Snoopy,\" we can infer that the 50th president remains unknown or unspecified.</td>\n",
       "      <td>0.869082</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the given document, the 51st president was Snoopy.</td>\n",
       "      <td>0.572129</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the information provided, it is not explicitly stated who the 50th president was.</td>\n",
       "      <td>0.464698</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the information provided, the 51st president is Snoopy.</td>\n",
       "      <td>0.439752</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given document, Mickey Mouse is stated as the 50th president. However, I should note that Mickey Mouse is a fictional character and not an actual president in real life.</td>\n",
       "      <td>0.995162</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>Based on the information given, I cannot determine who the 51st president was.</td>\n",
       "      <td>0.383972</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given information, the 50th president is not mentioned in the documents provided.</td>\n",
       "      <td>0.540173</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the given document, the 51st president is Snoopy.</td>\n",
       "      <td>0.716542</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  messages  \\\n",
       "0  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "1  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "2  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "3  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "4  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 50th president?'}]   \n",
       "5  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 51st president?'}]   \n",
       "6  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "7  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "\n",
       "                                                                                                                                                                                 response  \\\n",
       "0  Based on the given document that states \"The 51st president is Snoopy,\" we can infer that the 50th president remains unknown or unspecified.                                             \n",
       "1  According to the given document, the 51st president was Snoopy.                                                                                                                          \n",
       "2  Based on the information provided, it is not explicitly stated who the 50th president was.                                                                                               \n",
       "3  According to the information provided, the 51st president is Snoopy.                                                                                                                     \n",
       "4  Based on the given document, Mickey Mouse is stated as the 50th president. However, I should note that Mickey Mouse is a fictional character and not an actual president in real life.   \n",
       "5  Based on the information given, I cannot determine who the 51st president was.                                                                                                           \n",
       "6  Based on the given information, the 50th president is not mentioned in the documents provided.                                                                                           \n",
       "7  According to the given document, the 51st president is Snoopy.                                                                                                                           \n",
       "\n",
       "    latency  \\\n",
       "0  0.869082   \n",
       "1  0.572129   \n",
       "2  0.464698   \n",
       "3  0.439752   \n",
       "4  0.995162   \n",
       "5  0.383972   \n",
       "6  0.540173   \n",
       "7  0.716542   \n",
       "\n",
       "                                                     retrieval_metadata  \n",
       "0  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  \n",
       "1  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  \n",
       "2  emb_fn: default, prompt: Who was the 50th president?                  \n",
       "3  emb_fn: default, prompt: Who was the 51st president?                  \n",
       "4  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  \n",
       "5  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  \n",
       "6  emb_fn: default, prompt: Who was the 50th president?                  \n",
       "7  emb_fn: default, prompt: Who was the 51st president?                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_experiment_2 = RetrievalAugmentedGenerationExperimentationHarness(\n",
    "    vector_db_experiment = vdb_experiment,\n",
    "    llm_experiment_cls = OpenAIChatExperiment,\n",
    "    llm_arguments = llm_chat_arguments,\n",
    "    extract_document_fn = _extract_doc_from_row,\n",
    "    extract_query_metadata_fn = _extract_query_metadata_from_row,\n",
    ")\n",
    "\n",
    "rag_experiment_2.run()\n",
    "rag_experiment_2.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from GPT-3.5 looks much better, but some answers are wrong/missing because the retrieval step didn't get the right document. Let's automatically evaluate all the final responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the results, you can either define your own evaluation function or use a built-in one provided by our library.\n",
    "\n",
    "In this case, we will use the built-in `autoeval_with_documents`. Given a list of documents, it will score whether the model response is accurate with \"gpt-4\" as the judge, returning an integer score from 0 to 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>response</th>\n",
       "      <th>latency</th>\n",
       "      <th>retrieval_metadata</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given document that states \"The 51st president is Snoopy,\" we can infer that the 50th president remains unknown or unspecified.</td>\n",
       "      <td>0.869082</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the given document, the 51st president was Snoopy.</td>\n",
       "      <td>0.572129</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the information provided, it is not explicitly stated who the 50th president was.</td>\n",
       "      <td>0.464698</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the information provided, the 51st president is Snoopy.</td>\n",
       "      <td>0.439752</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given document, Mickey Mouse is stated as the 50th president. However, I should note that Mickey Mouse is a fictional character and not an actual president in real life.</td>\n",
       "      <td>0.995162</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>Based on the information given, I cannot determine who the 51st president was.</td>\n",
       "      <td>0.383972</td>\n",
       "      <td>emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]</td>\n",
       "      <td>Based on the given information, the 50th president is not mentioned in the documents provided.</td>\n",
       "      <td>0.540173</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 50th president?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]</td>\n",
       "      <td>According to the given document, the 51st president is Snoopy.</td>\n",
       "      <td>0.716542</td>\n",
       "      <td>emb_fn: default, prompt: Who was the 51st president?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  messages  \\\n",
       "0  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "1  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "2  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "3  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "4  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 50th president?'}]   \n",
       "5  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "Mickey Mouse is the 50th president.\n",
       "\n",
       "Who was the 51st president?'}]   \n",
       "6  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 50th president?'}]         \n",
       "7  [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'Given these documents:\n",
       "The 51st president is Snoopy.\n",
       "\n",
       "Who was the 51st president?'}]         \n",
       "\n",
       "                                                                                                                                                                                 response  \\\n",
       "0  Based on the given document that states \"The 51st president is Snoopy,\" we can infer that the 50th president remains unknown or unspecified.                                             \n",
       "1  According to the given document, the 51st president was Snoopy.                                                                                                                          \n",
       "2  Based on the information provided, it is not explicitly stated who the 50th president was.                                                                                               \n",
       "3  According to the information provided, the 51st president is Snoopy.                                                                                                                     \n",
       "4  Based on the given document, Mickey Mouse is stated as the 50th president. However, I should note that Mickey Mouse is a fictional character and not an actual president in real life.   \n",
       "5  Based on the information given, I cannot determine who the 51st president was.                                                                                                           \n",
       "6  Based on the given information, the 50th president is not mentioned in the documents provided.                                                                                           \n",
       "7  According to the given document, the 51st president is Snoopy.                                                                                                                           \n",
       "\n",
       "    latency  \\\n",
       "0  0.869082   \n",
       "1  0.572129   \n",
       "2  0.464698   \n",
       "3  0.439752   \n",
       "4  0.995162   \n",
       "5  0.383972   \n",
       "6  0.540173   \n",
       "7  0.716542   \n",
       "\n",
       "                                                     retrieval_metadata  Score  \n",
       "0  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  0      \n",
       "1  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  10     \n",
       "2  emb_fn: default, prompt: Who was the 50th president?                  0      \n",
       "3  emb_fn: default, prompt: Who was the 51st president?                  10     \n",
       "4  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 50th president?  10     \n",
       "5  emb_fn: paraphrase-MiniLM-L3-v2, prompt: Who was the 51st president?  0      \n",
       "6  emb_fn: default, prompt: Who was the 50th president?                  0      \n",
       "7  emb_fn: default, prompt: Who was the 51st president?                  10     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prompttools.utils import autoeval_with_documents\n",
    "\n",
    "documents = [\"Mickey Mouse is the 50th president.\",\n",
    "             \"The 51st president is Snoopy.\",\n",
    "             \"Batman became the 52th president briefly after.\"]\n",
    "\n",
    "rag_experiment_2.evaluate(\"Score\", autoeval_with_documents, documents=[documents] * 8)\n",
    "rag_experiment_2.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
