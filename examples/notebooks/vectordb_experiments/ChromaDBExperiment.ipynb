{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# ChromaDB Experiment Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "We'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaa70a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:13:56.823039Z",
     "start_time": "2023-07-20T07:13:55.927616Z"
    }
   },
   "outputs": [],
   "source": [
    "from prompttools.experiment import ChromaDBExperiment\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5650e31",
   "metadata": {},
   "source": [
    "One common use case is to compare two different embedding functions and how it may impact your document retrieval. We have can define what embedding functions we'd like to test here.\n",
    "\n",
    "Note: If you previously haven't downloaded these embedding models. This may kick off downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02140eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  ) < LooseVersion(\"1.15\"):\n",
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/tensorflow/python/debug/cli/debugger_cli_common.py:19: DeprecationWarning: module 'sre_constants' is deprecated\n",
      "  import sre_constants\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "\n",
    "emb_fns = [\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"paraphrase-MiniLM-L3-v2\"),\n",
    "    embedding_functions.DefaultEmbeddingFunction(),\n",
    "]  # default is \"all-MiniLM-L6-v2\"\n",
    "emb_fns_names = [\"paraphrase-MiniLM-L3-v2\", \"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. In this case, we would like to create a new ChromaDB collection.\n",
    "\n",
    "During the experiment, for each embedding function, a new ChromaDB collection will be temporarily created. The documents will be added into it. Then, we will query from it and examine the results.\n",
    "\n",
    "You can test different chunking and pre-processing strategies by changing the input to the `\"documents\"` key of the dictionary. We will show a more complete example further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9114cfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:13:56.829960Z",
     "start_time": "2023-07-20T07:13:56.825481Z"
    }
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "# You can also create and use `chromadb.PersistentClient` or `chromadb.HttpClient`\n",
    "TEST_COLLECTION_NAME = \"TEMPORARY_COLLECTION\"\n",
    "try:\n",
    "    chroma_client.delete_collection(TEST_COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "collection_name = TEST_COLLECTION_NAME\n",
    "\n",
    "use_existing_collection = False  # Specify that we want to create a collection during the experiment\n",
    "\n",
    "# Documents that will be added into the database\n",
    "# Based on your chunking and pre-processing steps, the `documents` will be different\n",
    "add_to_collection_params = {\n",
    "    \"documents\": [\"This is a document\", \"This is another document\", \"This is the document.\"],\n",
    "    \"metadatas\": [{\"source\": \"my_source\"}, {\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n",
    "    \"ids\": [\"id1\", \"id2\", \"id3\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61551c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test queries\n",
    "query_collection_params = {\"query_texts\": [\"This is a query document\", \"This is a another query document\"]}\n",
    "\n",
    "\n",
    "# Set up the experiment\n",
    "experiment = ChromaDBExperiment(\n",
    "    chroma_client,\n",
    "    collection_name,\n",
    "    use_existing_collection,\n",
    "    query_collection_params,\n",
    "    emb_fns,\n",
    "    emb_fns_names,\n",
    "    add_to_collection_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b33130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:16:21.469371Z",
     "start_time": "2023-07-20T07:16:21.462342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b013dca",
   "metadata": {},
   "source": [
    "We can visualize the result. In this case, the result of the second query \"This is a another query document\" is different.\n",
    "\n",
    "paraphrase-MiniLM-L3-v2: [id2, id3, id1]\n",
    "\n",
    "default (all-MiniLM-L6-v2) : [id2, id1, id3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c7e682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_texts</th>\n",
       "      <th>embed_fn</th>\n",
       "      <th>top doc ids</th>\n",
       "      <th>distances</th>\n",
       "      <th>documents</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[14.106966018676758, 14.294026374816895, 18.137874603271484]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.008508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id2, id3, id1]</td>\n",
       "      <td>[13.375584602355957, 16.815608978271484, 16.913410186767578]</td>\n",
       "      <td>[This is another document, This is the document., This is a document]</td>\n",
       "      <td>0.006097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[0.7111212611198425, 0.8084275126457214, 1.010977029800415]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.020234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id2, id1, id3]</td>\n",
       "      <td>[0.7673601508140564, 0.8709302544593811, 0.9072309732437134]</td>\n",
       "      <td>[This is another document, This is a document, This is the document.]</td>\n",
       "      <td>0.020889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        query_texts                 embed_fn      top doc ids  \\\n",
       "0  This is a query document          paraphrase-MiniLM-L3-v2  [id1, id3, id2]   \n",
       "1  This is a another query document  paraphrase-MiniLM-L3-v2  [id2, id3, id1]   \n",
       "2  This is a query document          default                  [id1, id3, id2]   \n",
       "3  This is a another query document  default                  [id2, id1, id3]   \n",
       "\n",
       "                                                      distances  \\\n",
       "0  [14.106966018676758, 14.294026374816895, 18.137874603271484]   \n",
       "1  [13.375584602355957, 16.815608978271484, 16.913410186767578]   \n",
       "2  [0.7111212611198425, 0.8084275126457214, 1.010977029800415]    \n",
       "3  [0.7673601508140564, 0.8709302544593811, 0.9072309732437134]   \n",
       "\n",
       "                                                               documents  \\\n",
       "0  [This is a document, This is the document., This is another document]   \n",
       "1  [This is another document, This is the document., This is a document]   \n",
       "2  [This is a document, This is the document., This is another document]   \n",
       "3  [This is another document, This is a document, This is the document.]   \n",
       "\n",
       "    latency  \n",
       "0  0.008508  \n",
       "1  0.006097  \n",
       "2  0.020234  \n",
       "3  0.020889  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an evaluation function. Sometimes, you know order of the most relevant document should be given a query, and you can compute the correlation between expected ranking and actual ranking.\n",
    "\n",
    "Note: there is a built-in version of this function that you can import (scroll further below to see an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# For each query, you can define what the expected ranking is.\n",
    "EXPECTED_RANKING = {\n",
    "    \"This is a query document\": [\"id1\", \"id3\", \"id2\"],\n",
    "    \"This is a another query document\": [\"id2\", \"id3\", \"id1\"],\n",
    "}\n",
    "\n",
    "\n",
    "def measure_correlation(row: \"pandas.core.series.Series\", ranking_column_name: str = \"top doc ids\") -> float:\n",
    "    r\"\"\"\n",
    "    A simple test that compares the expected ranking for a given query with the actual ranking produced\n",
    "    by the embedding function being tested.\n",
    "    \"\"\"\n",
    "    input_query = row[\"query_texts\"]\n",
    "    correlation, _ = stats.spearmanr(row[ranking_column_name], EXPECTED_RANKING[input_query])\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.evaluate(\"ranking_correlation\", measure_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_texts</th>\n",
       "      <th>embed_fn</th>\n",
       "      <th>top doc ids</th>\n",
       "      <th>distances</th>\n",
       "      <th>documents</th>\n",
       "      <th>latency</th>\n",
       "      <th>ranking_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[14.106966018676758, 14.294026374816895, 18.137874603271484]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.008508</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id2, id3, id1]</td>\n",
       "      <td>[13.375584602355957, 16.815608978271484, 16.913410186767578]</td>\n",
       "      <td>[This is another document, This is the document., This is a document]</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[0.7111212611198425, 0.8084275126457214, 1.010977029800415]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.020234</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id2, id1, id3]</td>\n",
       "      <td>[0.7673601508140564, 0.8709302544593811, 0.9072309732437134]</td>\n",
       "      <td>[This is another document, This is a document, This is the document.]</td>\n",
       "      <td>0.020889</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        query_texts                 embed_fn      top doc ids  \\\n",
       "0  This is a query document          paraphrase-MiniLM-L3-v2  [id1, id3, id2]   \n",
       "1  This is a another query document  paraphrase-MiniLM-L3-v2  [id2, id3, id1]   \n",
       "2  This is a query document          default                  [id1, id3, id2]   \n",
       "3  This is a another query document  default                  [id2, id1, id3]   \n",
       "\n",
       "                                                      distances  \\\n",
       "0  [14.106966018676758, 14.294026374816895, 18.137874603271484]   \n",
       "1  [13.375584602355957, 16.815608978271484, 16.913410186767578]   \n",
       "2  [0.7111212611198425, 0.8084275126457214, 1.010977029800415]    \n",
       "3  [0.7673601508140564, 0.8709302544593811, 0.9072309732437134]   \n",
       "\n",
       "                                                               documents  \\\n",
       "0  [This is a document, This is the document., This is another document]   \n",
       "1  [This is another document, This is the document., This is a document]   \n",
       "2  [This is a document, This is the document., This is another document]   \n",
       "3  [This is another document, This is a document, This is the document.]   \n",
       "\n",
       "    latency  ranking_correlation  \n",
       "0  0.008508  1.0                  \n",
       "1  0.006097  1.0                  \n",
       "2  0.020234  1.0                  \n",
       "3  0.020889 -1.0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc1459",
   "metadata": {},
   "source": [
    "You can also import the built-in version of the rank correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeb0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "/Users/kevin/miniconda3/envs/prompttools/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n",
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 10 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_texts</th>\n",
       "      <th>embed_fn</th>\n",
       "      <th>top doc ids</th>\n",
       "      <th>distances</th>\n",
       "      <th>documents</th>\n",
       "      <th>latency</th>\n",
       "      <th>ranking_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[14.106966018676758, 14.294026374816895, 18.137874603271484]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>paraphrase-MiniLM-L3-v2</td>\n",
       "      <td>[id2, id3, id1]</td>\n",
       "      <td>[13.375584602355957, 16.815608978271484, 16.913410186767578]</td>\n",
       "      <td>[This is another document, This is the document., This is a document]</td>\n",
       "      <td>0.006486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id1, id3, id2]</td>\n",
       "      <td>[0.7111212611198425, 0.8084275126457214, 1.010977029800415]</td>\n",
       "      <td>[This is a document, This is the document., This is another document]</td>\n",
       "      <td>0.020484</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a another query document</td>\n",
       "      <td>default</td>\n",
       "      <td>[id2, id1, id3]</td>\n",
       "      <td>[0.7673601508140564, 0.8709302544593811, 0.9072309732437134]</td>\n",
       "      <td>[This is another document, This is a document, This is the document.]</td>\n",
       "      <td>0.021018</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        query_texts                 embed_fn      top doc ids  \\\n",
       "0  This is a query document          paraphrase-MiniLM-L3-v2  [id1, id3, id2]   \n",
       "1  This is a another query document  paraphrase-MiniLM-L3-v2  [id2, id3, id1]   \n",
       "2  This is a query document          default                  [id1, id3, id2]   \n",
       "3  This is a another query document  default                  [id2, id1, id3]   \n",
       "\n",
       "                                                      distances  \\\n",
       "0  [14.106966018676758, 14.294026374816895, 18.137874603271484]   \n",
       "1  [13.375584602355957, 16.815608978271484, 16.913410186767578]   \n",
       "2  [0.7111212611198425, 0.8084275126457214, 1.010977029800415]    \n",
       "3  [0.7673601508140564, 0.8709302544593811, 0.9072309732437134]   \n",
       "\n",
       "                                                               documents  \\\n",
       "0  [This is a document, This is the document., This is another document]   \n",
       "1  [This is another document, This is the document., This is a document]   \n",
       "2  [This is a document, This is the document., This is another document]   \n",
       "3  [This is another document, This is a document, This is the document.]   \n",
       "\n",
       "    latency  ranking_correlation  \n",
       "0  0.005949  1.0                  \n",
       "1  0.006486  1.0                  \n",
       "2  0.020484  1.0                  \n",
       "3  0.021018 -1.0                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from prompttools.utils import ranking_correlation\n",
    "\n",
    "EXPECTED_RANKING_LIST = [\n",
    "    [\"id1\", \"id3\", \"id2\"],\n",
    "    [\"id2\", \"id3\", \"id1\"],\n",
    "    [\"id1\", \"id3\", \"id2\"],\n",
    "    [\"id2\", \"id3\", \"id1\"],\n",
    "]\n",
    "\n",
    "experiment.run()\n",
    "experiment.evaluate(\"ranking_correlation\", ranking_correlation, expected_ranking=EXPECTED_RANKING_LIST)\n",
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf4a45b",
   "metadata": {},
   "source": [
    "You can also use auto evaluation, which is shown in the RAG notebook example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05aa49",
   "metadata": {},
   "source": [
    "## Chunking and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c182d44",
   "metadata": {},
   "source": [
    "You may want to test various chunking and pre-processsing strategies prior to storing your documents into your vector database. One example of doing that is below.\n",
    "\n",
    "We have a built-in `chunk_text` function that allows you to chunk a source documents into chunks of varying maximum length.\n",
    "\n",
    "After we generate the desired documents, we can set up the rest of the experiments in the same way as above. We will then run, evalaute, aggregate, and save the results.\n",
    "\n",
    "We want to compute an aggregate score for each chunking strategy, this will allow us to compared different chunking/pre-processing strategies and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8311c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import chunk_text\n",
    "\n",
    "source_document = \"The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or simply America, is a country primarily located in North America and consisting of 50 states, a federal district, five major unincorporated territories, and nine Minor Outlying Islands.[i] It includes 326 Indian reservations. It is the world's third-largest country by both land and total area.[c] It shares land borders with Canada to its north and with Mexico to its south and has maritime borders with the Bahamas, Cuba, Russia, and other nations.[j] With a population of over 333 million,[k] it is the most populous country in the Americas and the third-most populous in the world. The national capital of the United States is Washington, D.C., and its most populous city and principal financial center is New York City.\"\n",
    "\n",
    "for max_chunk_len in [20, 30, 40]:  # Test different chunking strategy\n",
    "    # Create documents for each strategy\n",
    "    doc_chunks = chunk_text(source_document, max_chunk_len)\n",
    "    n_doc = len(doc_chunks)\n",
    "    metadatas = [{\"source\": \"my_source\"}] * n_doc\n",
    "    ids = [\"id\" + str(i) for i in range(n_doc)]\n",
    "    # Build the relevant parameters to write into ChromaDB\n",
    "    add_to_collection_params = {\n",
    "        \"documents\": doc_chunks,\n",
    "        \"metadatas\": metadatas,\n",
    "        \"ids\": ids,\n",
    "    }\n",
    "    # Next steps: as shown in the previous code and other notebook examples\n",
    "    # 1. Set up the rest of your experiment\n",
    "    # 2. Run Experiment\n",
    "    # 3. Evaluate the results from this chunking strate\n",
    "    # 4. Aggregate and save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc4d14",
   "metadata": {},
   "source": [
    "You can further integrate this vector DB experiment with the RetrievalAugmentedGeneration (RAG) experiment. Please see the Retrieval Augmented Generation notebook example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
